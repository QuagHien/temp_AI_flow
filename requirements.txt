# embeding
torch==1.13
transformers
einops
timm
# flash_attn==1.0.5
fastembed
#fastembed-gpu

# VLLM
git+https://github.com/huggingface/transformers
qwen-vl-utils
accelerate>=0.26.0
bitsandbytes
optimum
auto-gptq